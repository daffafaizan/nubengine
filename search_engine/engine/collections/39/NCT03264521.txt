
    
      The study aims to advance the pain research by exploring the crowdsourcing approach for
      eliciting and analyzing the way in which individuals experience and understand chronic pain.
      Investigators will leverage an existing NIH/NCCIH grant (RAND Center of Excellence in
      Research on CAM; CERC) to conduct a feasibility study of new methods for gathering and
      analyzing data on chronic pain and engaging pain patients in health policy processes through
      three specific aims:

      Aim 1 (Inclusion): Gain access to chronic pain patients using crowdsourcing platform Amazon
      Mechanical Turk (MTurk). This aim explores whether crowdsourcing provides a credible method
      for patient inclusion. People with low back pain will be accessed via the crowdsourcing
      platform MTurk and asked to take health surveys that were also administered to a national
      clinical sample of chiropractic patients within a RAND study. Equivalency of validated,
      self-reported measures of low back pain obtained from crowdsourced versus "gold standard"
      data from the RAND study will be assessed. Similarities and differences between demographics
      and other pain and function variables between crowdsourced and RAND data will be analyzed.
      Subsamples of crowdsourced data will be analyzed to assess reliability of the extent to which
      data yields the same results across repeated crowdsourced samples.

      Aim 2 (Participation): Engage chronic pain patients in inclusion criteria setting for
      national pain treatment programs. This aim will intends to facilitate patient participation
      in NIH criteria-setting for program inclusion. Crowdsourced patients will assist with
      qualitative coding of data responses to the question, "What does chronic pain mean to you?"
      Investigators will explore whether crowdsourcing provides a valid method by which coding may
      be conducted, first measuring reliability across crowd samples, second testing the accuracy
      of participant coding as compared with expert coders at RAND Corporation. A method of
      assessing face validity will be tested as participants may create additional codes and give
      feedback by rating the importance of each dimension.

      Aim 3: Assess efficiency and quality of crowdsourced data as compared to CERC data.
      Investigators will draw quantitative comparisons of cost (labor/incentives), time, data
      quality (amount of text, missing data) across online crowdsourced and CERC study samples.

      The proposed study utilizes the resources of an existing NIH grant by exploring the
      feasibility of using innovative online methods for eliciting patient perspectives on chronic
      pain and for engaging patients in analyses procedures. The study provides an opportunity to
      determine whether patient chronic pain experiences and perspectives can be gathered through
      crowdsourcing using Amazon Mechanical Turk (MTurk), in a valid, replicable, and resource
      efficient way. Although focused on chronic low back pain, the study findings will have broad
      implications for patient engagement more generally. If the crowdsourcing methods produce data
      that is comparable to "gold standard" methods used in the RAND Study entitled, "Center of
      Excellence in Research on Chiropractic" (RAND/CERC Study), this new experimental system has
      the potential to provide low-cost and time-efficient methods to advance
      democratically-oriented research, evaluation, policy and ultimately patient-centered clinical
      care.
    
  