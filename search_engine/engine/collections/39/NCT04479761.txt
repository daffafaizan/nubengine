
    
      Introduction: Aim 1 is to establish the role of generated and natural sounds in postural
      control given the visual environment and sensory loss. For that the investigators will
      measure postural sway in individuals with unilateral peripheral vestibular hypofunction
      (n=45), individuals with SSD (n=45) and age-matched controls (n=45). They will be tested in
      an immersive virtual reality environment displaying an abstract 3-wall display of stars or a
      subway station. Within each environment, we will compare changes in postural sway in response
      to visual (static, dynamic) and auditory perturbations (no sound, dynamic sound, i.e.,
      rhythmic white noise in the stars environment or natural sounds, such as moving trains, in
      the subway environment). Aim 2 is to determine the extent to which a static white noise can
      improve balance (reduce postural sway) within a dynamic visual environment in individuals
      with and without sensory loss. To accomplish this aim, the 3 groups of participants will be
      tested within the same visual environment but here we will compare their sway within a
      sound-free dynamic visual environment to that with static white noise.

      System: Visuals were designed in C# language using standard Unity Engine version
      2018.1.8f1(64-bit) (©Unity Tech., San Francisco, CA, USA). The scenes will be delivered via
      an HTC Vive headset (Taoyuan City, Tai-wan) controlled by a Dell Alienware laptop 15 R3
      (Round Rock, TX, USA). The Vive has built-in positional track-ing operating at 60Hz and a
      refresh rate at 90 Hz. Sounds will be delivered via Bose (Bose Corporation, Fram-ingham, MA,
      USA) QuietComfort 35 wireless headphones II with active noise cancellation and 360º spatial
      audio. The process of creating auditory cues included over 20 hours of sound field recording
      based on the targeted scenes and their intensity levels in New York City. Auditory cues were
      captured with the Sennheiser Ambeo microphone in first order Ambisonics format. The
      background sounds merged with a sound design process which involved simulating the detailed
      environmental sounds that exist within the natural environment to develop a real-world sonic
      representation. The audio files were processed in Wwise and integrated into Unity. Postural
      sway will be recorded at 100 Hz by Qualisys software for a Kistler 5233A force-platform
      (Winterthur, Switzer-land).

      Data Collection: Potentially eligible participants will complete a demographics form and go
      through the following diagnostic screening at the Ear Institute: Caloric Test, Video Head
      Impulse Test (vHIT), Ocular / Cervical Vestibular Evoked Myogenic Potential, and Audiogram.
      Visual and somatosensory screening will be done at the Ear Institute as well. This first
      session is expected to take 2.5 hours to complete. Participants will receive questionnaires
      to complete at home or on the next session. The Dizziness Handicap Inventory (DHI) was
      designed to identify difficulties that a patient may be experiencing because of dizziness.
      The Activities-Specific Balance Confidence (ABC) is a measure of confidence in performing
      various ambulatory activities without falling or feeling 'unsteady'. The State-Trait Anxiety
      Inventory (STAI) assesses the severity of anxiety symptoms and a generalized tendency to be
      anxious. The Speech, Spatial and Quality of Hearing 12-item Scale (SSQ12) is a valid, short
      version of the original SSQ which provides insights on day-to-day hearing loss impact. The
      virtual reality protocol (testing by the PI at the NYU Human Performance Laboratory) includes
      12 conditions: 2 environments (an abstract display of stars, a subway station) X 2 visuals
      (moving, static) X 3 sounds (dynamic, none, static white noise) each repeated 3 times for a
      total of 36 trials. It will be randomized and completed over 1-2 sessions, as needed of up to
      90 minutes each. Sounds will be played at the highest level that is comfortable to the
      participant. Scenes are 60 seconds long. Throughout all sessions, the patients will complete
      the Simulator Sickness Questionnaire, used to monitor participants' symptoms.

      Data Analysis: For each of the 3 measures of interest and for each environment, we will fit a
      linear mixed effects model. Each model will include main effects of group, visual condition,
      and auditory condition, as well as all 2 and 3-way interactions. The models will also control
      for caloric and Video Head Impulse Test (vHIT) test results as well as Age Related Hearing
      Loss and age. For aim 1, we will assess the significance of contrasts between no sounds /
      dynamic sounds for the different visual conditions and groups. For aim 2, the same will be
      done for contrasts between no sounds / static sounds. These models estimate the difference in
      visual weighting and reweighting between the groups, maximizing the information we can obtain
      from the data by accounting for the inherent multi-level study design (person, conditions,
      repetitions). Since each person completes various trials for each condition, the linear mixed
      effects model accounts for these sources of variability. P-values for the fixed effects will
      be calculated using the Satterthwaite approximation for the degrees of freedom for the
      T-distribution80. In addition, we will descriptively explore the relationship between DP,
      area, self-reported outcomes (DHI, ABC, STAI, SSQ12), and age.
    
  