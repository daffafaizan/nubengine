
    
      Autism Spectrum Disorder (ASD) is a neurodevelopmental communication disorder resulting in
      functional language and behavioral delays affecting over 3.5 million Americans. These delays
      vary with the severity of symptoms that present in ASD but often result in limited speech and
      increased communication challenges. Alongside linguistic acquisition, oral motor coordination
      is a crucial part of speech production.

      Current clinical techniques have shown varying degrees of efficacy in improving functional
      language proficiency. Most techniques follow a drill-like procedure, where the child is made
      to repeat various sounds and phrases until they are retained. However, such a process
      requires potentially over twenty therapy sessions to show improvement which may then only be
      focused on one aspect of speech. This significantly limits the linguistic and social skills a
      student will acquire. To improve the efficacy of these therapy sessions, new technology must
      be developed to provide the most effective educational experience.

      Video-assisted speech technology (VAST) is a method of using a video of a close-up model of
      the mouth and speaking simultaneously with it. Rather than present the individual with a
      static photograph of the initial phoneme, the entire sequence of oral movements can be
      presented sequentially via video-recorded segments of the orofacial area producing connected
      speech, combining best practices, video modeling, and literacy with auditory cues to provide
      unprecedented support the development of vocabulary, word combinations and communication.

      In this SBIR Phase I proposal, iTherapy will develop a personalized educational experience
      for students with ASD by creating a virtual reality (VR) based VAST program to stimulate
      engagement and speech production practice. VR offers several benefits as a therapy technique:
      overcoming sensory difficulties, more effectively generalizing information, employing visual
      learning, and providing individualized treatment. As a user moves through the stages of the
      program, they will be immersed in a proactive environment where they will engross themselves
      with continuous content.

      Rather than present the individual with a static photograph of the initial phoneme, the
      entire sequence of oral movements can be presented sequentially via VR-modelled segments of
      the orofacial area producing connected speech, combining best practices, video modeling,
      music therapy, and literacy with auditory cues to provide unprecedented support the
      development of vocabulary, word combinations and communication. The innovation will be a
      video series of a realistic VR mouth which will require the use of an app on a tablet or a
      smartphone, VR goggles, and bone conduction headphones.
    
  